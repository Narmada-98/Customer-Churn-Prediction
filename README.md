# Customer Churn Prediction

## Overview

This project predicts **customer churn** using machine learning techniques on the [Telco Customer Churn dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn). The objective is to identify customers likely to discontinue services, enabling proactive retention strategies.

## Features

* **Exploratory Data Analysis (EDA)** â€“ Insights into customer behavior and churn patterns
* **Data Preprocessing** â€“ Handling missing values, encoding categorical variables
* **Class Imbalance Handling** â€“ **SMOTE**
* **Model Selection** â€“ Comparison of Decision Tree, Random Forest, and XGBoost
* **Hyperparameter Tuning** â€“ **Optuna** 
* **Cross-Validation** â€“ K-Fold for robust evaluation
* **Model Evaluation** â€“ ROC-AUC, Precision, Recall, F1-Score, and confusion matrices

## Project Structure

```text
customer-churn-prediction/
â”‚
â”œâ”€â”€ data/                         # Dataset files 
â”œâ”€â”€ 1_data_exploration.ipynb      # EDA: distributions, missing values, correlations
â”œâ”€â”€ 2_data_preprocessing.ipynb    # Data cleaning, encoding, SMOTE, and splitting
â”œâ”€â”€ 3_modelling_and_hyperparameter_tuning.ipynb  # Cross-validation & Optuna tuning
â”œâ”€â”€ 4_evaluation_and_interpretation.ipynb        # Metrics, ROC-AUC, and confusion matrices
â”œâ”€â”€ models/                       # Model files
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md                     # Project documentation
```

## How to Run

### 1ï¸âƒ£ **Clone the repository**

```bash
git clone https://github.com/Narmada-98/Customer-Churn-Prediction.git
cd customer-churn-prediction
```

### 2ï¸âƒ£ **Create a virtual environment and install dependencies**

```bash
python -m venv venv
source venv/bin/activate      # On Mac/Linux
venv\Scripts\activate         # On Windows
pip install -r requirements.txt
```

### 3ï¸âƒ£ **Run the notebooks in order**

1. `1_data_exploration.ipynb` â€“ Perform EDA and note insights
2. `2_data_preprocessing.ipynb` â€“ Clean and prepare the data
3. `3_modelling_and_hyperparameter_tuning.ipynb` â€“ Train and tune models
4. `4_evaluation_and_interpretation.ipynb` â€“ Evaluate and interpret results

## ğŸ“ˆ Results

* **Best Model**: Random Forest (AUC â‰ˆ 0.85)

## ğŸ§° Technologies Used

* **Programming Language**: Python
* **Libraries**: pandas, NumPy, scikit-learn, imbalanced-learn, Optuna, seaborn, matplotlib

